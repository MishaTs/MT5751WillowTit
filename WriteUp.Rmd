---
title: "Tit for Tat: Contravening Drivers of Swiss Willow Tit Occupancy"
author: "Misha Tseitlin"
date: "`r format(Sys.Date(), format='%d %B %Y')`"
fontsize: 11pt
urlcolor: blue
bibliography: Bibliography.bib
csl: nature.csl
output: 
  bookdown::pdf_document2:
    toc: false
    number_sections: false
papersize: a4
header-includes:
  - \usepackage{float}
  - \usepackage{sectsty}
  - \usepackage{paralist}
  - \usepackage{fancyhdr}
  - \usepackage{dcolumn}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(statsecol)
library(ggplot2)
library(dplyr)
library(Distance)
library(kableExtra)
data(bowhead_LT)
load("dfModels.RData")

```

\subsectionfont{\raggedright}
\subsubsectionfont{\raggedright}

\pagenumbering{gobble}

\begin{centering}

\Large
{\bf Project 3: Occupancy Modelling}

\vspace{4cm}

```{r uni_logo, echo=F, out.width="50%"}
knitr::include_graphics("01-standard-vertical-black-text.png")

```

{\bf School of Mathematics and Statistics}

\vspace{1 cm}

\normalsize
in partial fulfilment of the requirements for \\
MT5751: Estimating Animal Abundance and Biodiversity \\

\end{centering}

\newpage
\pagenumbering{arabic} 

# Executive Summary

Bowhead whales (*Balaena mysticetus*) aggregate each spring to mate in west Greenland's Disko Bay (Qeqertarsuaq), providing a rare chance to measure this highly mobile species' abundance. Distance sampling (DS) uses likelihood estimation to generate estimates about abundance while accounting for imperfect detectability and has previously estimated bowhead whale numbers around Qeqertarsuaq. We analyse existing aerial survey data with simple DS methods to test the robustness of estimates without incorporating whale surfacing behaviour.[@rekdal_2014_trends] As data appears to violate DS assumptions, we compare different data processing approaches and their impact on size effect and abundance estimates. Our simple DS, even after including all possible covariates, underestimates abundance relative to the original approach. Though DS remains a powerful tool to understand population sizes, we must approach cases of small sample sizes and simplified secondary analyses with caution.

# Introduction
Annual springtime bowhead whale aggregations in Qeqertarsuaq allow for analysing both subpopulation stock rebound after whaling moratoria and estimating baseline population before wider climate change impacts in the Arctic. Using existing visual aerial surveys, we conduct a simplified replication of existing DS analysis using only covariates and adjustment terms to test method robustness and data suitability for more accessible methods for understanding bowhead whale detection, abundance, and impacts of group size. 

# Methods
Distance sampling estimates animal abundance using perpendicular distances between the transect and observed animals. A detection function $g(x)$ is fitted to calculate the probability of detecting an animal at a distance $x$ and account for imperfect observations. Based on rigid half-normal (HN) and hazard rate (HR) key functions, we can incorporate covariates to model differences in detectability.[@buckland_2015_distance] As $\sigma>0$, we use a log link function. So,
$$\sigma(\mathbf{z}_i) = exp(\alpha_0 + \sum_{j=1}^n \alpha_jz_{ij})$$
where $\mathbf{z}_i = (z_{i1},...,z_{in})$ is the vector of covariates for animal $i$ and $\alpha_0,...,\alpha_n$ represent relevant covariate parameters. Another approach to flexibility, adjustment terms, can further improve fit.

We then derive abundance estimates from the detection function: after accounting for stratified survey design, the detection probability is multiplied by encounter rate and a known constant. Variance here results first from detection function uncertainty, which we approximate using the delta method.[@borchers_horvitz-thompson_1998] Then, we incorporate the encounter rate uncertainty based on variance in transect-specific whale abundance estimates relative to survey effort (e.g., transect length) and covariate effects (e.g., group size).[@marques_covariate_2004] Though bootstrapped estimates can supplant delta method approximations, small sample size complicates their interpretation here.

After splitting Qeqertarsuaq into 16 strata and 41 systematically-placed east-west transects (4445km combined length), surveying used planes with two observation platforms to record declination angles and times of the first abeam whale sighting---generating perpendicular distances for DS---Beaufort sea state, and whale group size. Meaning resolved any differences in size or declination between observers. Authors left-truncated distances by 100 m due to obscured view close to the transect line, so we retained 58 observations of 74 whales.[@rekdal_2014_trends]

```{r Initial-histogram, echo = F, fig.height=4, fig.cap="Original bowhead whale detection distances"}
ggplot(data = bowhead_LT) + 
  geom_histogram(aes(x = distance, fill = as.factor(size)), 
                 breaks = seq(0,2.4, length.out = 12)) +
  labs(x = "Distance left-truncated by 0.1 km",
       y = "Observed # individuals",
       title = "Raw bowhead whale observations",
       subtitle = "Using 12 bins and minor left-truncated distances",
       fill = "Group size") + 
  theme_bw()
```

As-is detection distances demonstrate possible line avoidance, where more whales appear further from the plane than on the transect (Figure \@ref(fig:Initial-histogram)). DS assumptions---independent animal distribution with respect to the transect and perfect detection at distance 0---may not hold if whales hid and/or moved.[@buckland_2015_distance] However, a difference of only 4 observations between 0--0.2 and 0.4--0.6 km bins could be typical stochastic variation. Initially, we accepted this random variation and fit detection functions as-is. Though we avoid data loss, this method may overestimate detection probabilities (thus underestimating abundance) without a perfect detectability baseline distance.

Given the potential assumption violations, we considered two data adjustments. First, we grouped distances into progressively smaller bins with fewer observations in subsequent groupings (Figure 2). This presumes whales moved away from transect lines and were instead seen further away. Though accounting for potential assumption violations, our grouping copiously discards information. Our second option left-truncated distances until peak detectability at
0.74 km. Thus, we ensure a strict decline in detectability with increasing distance in exchange for discarding 20 more observations (25.7% of our total). Now accounting for potential hiding behaviours (e.g., diving), we still must assume perfect detectability at our new baseline distanceâ€”
0.74 km. Interestingly, this data better fit complex models with multiple covariates and adjustment terms at the risk of over-fitting and poor generalisability.

```{r binned-histogram, echo = F,fig.height=4, fig.cap = "Original bowhead whale detection distances with custom bins"}
histBreaks2 <- c(0, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, max(bowhead_LT$distance, 
                                                      na.rm = TRUE))
ggplot(data = bowhead_LT) + 
  geom_histogram(aes(x = distance, fill = as.factor(size)), breaks = histBreaks2) +
  labs(x = "Distance left-truncated by 0.1 km",
       y = "Observerd # Individuals",
       title = "Binned bowhead whale observations",
       subtitle = "Using 7 custom bins with minor left-truncated distances",
       fill = "Group size") + 
  theme_bw()
```

```{r truncatedHistogram, fig.height=4, fig.cap = "More truncated bowhead whale detection distances"}
bowhead_LT_Trunc = bowhead_LT %>% filter(distance >= 0.64) %>%
  mutate(distance = distance - 0.64)
ggplot(data = bowhead_LT_Trunc) + 
  geom_histogram(aes(x = distance, fill = as.factor(size)),
                 bins = 14) +
  labs(x = "Distance left-truncated by 0.74 km",
       y = "Observerd # Individuals",
       title = "Filtered bowhead whale observations",
       subtitle = "Using 14 bins with substantially left-truncated distances",
       fill = "Group size") + 
  theme_bw()
```

We used the `Distance` package in `R` to fit models using a maximum likelihood approach.[@miller_2019_distance] For each dataset (unedited, binned, truncated) we fitted HN and HR detection functions with all combinations of adjustment terms (i.e., cosine, Hermite, polynomial) and covariates. Aikake Information Criterion (AIC) compared between models using the same manipulated data.
 
# Results

```{r table1, echo = FALSE}
table <- data.frame(
	`Detection Function` = c("Half Normal", "Half Normal", "Half Normal", "Half Normal", "Hazard Rate", "Half Normal", "Half Normal"),
	Data = c("Base","Base","Binned","Binned","Truncated","Truncated","Truncated"),
	`Size Covariate` = c("Yes","No","Yes","No","Yes","No","Yes"),
	Estimate = c(212.07, 228.82, 212.84, 229.54, 1803.36, 519.51, 608.75),
	SE = c(65.62, 71.45, 66.14, 72.22, 1346.22, 90.74, 154.26),
	`Lower CI` = c(112.13, 122.00, 112.32, 121.94, 461.56, 363.25, 356.93),
	`Upper CI` = c(401.06, 429.14, 403.31, 432.07, 7045.88, 743.00, 1038.20),
	AIC = c(86.03, 86.32, 197.58, 197.91, 27.20, 28.43, 28.73)
)

# make sure all table contents are justified the same way
# fix column names
knitr::kable(table, align="c",
             format = "latex",
             col.names = c("Key Function",
                           "Dataset" ,
                           "Size Incl?",
                           "Abundance",
                           "SE",
                           "Lower CI",
                           "Upper CI",
                           "AIC"),
             caption = "Estimated abundance from best-performing models") %>% 
   kable_classic(full_width = F)

```

```{r table2, echo = FALSE}
table2 <- data.frame(
	Model = c("Base HN + size", "Binned HN + size", "Truncated HN + size"),
	`Size Effect` = c(6.16, 5.64, -0.51),
	SE = c(5046.40, 305.31, 0.26)
)

knitr::kable(table2, 
             align="c",
             format = "latex",
             col.names = c("Model" ,
                           "Size Effect Magnitude",
                           "Standard Error"),
             caption = "Size effects on detectability") %>% 
   kable_classic(full_width = F)
```

For the base dataset, AIC preferred a HN detection function with a size covariate and no adjustments which estimated abundance as 212 individuals, only slightly lower than the non-covariate model after accounting for stratified survey design. Binned data produced near-identical results, with a preferred HN function including size. In contrast, truncated data estimated substantially higher abundance both for HR and HN models; despite large standard errors, these better matched primary analysis (Table \@ref(tab:table1)). The effect of size also remains unclear---original data suggests larger group sizes improve detectability while truncated data implies the converse (Table \@ref(tab:table2)). Because truncated modelling with HN detection suggests worse fit including size, we side with improved detectability after accounting for size bias: higher detectability for larger groups implies artificially-inflated abundance stemming from overconfidence in detectability for single individuals.

Fitted detection functions confirm this; original data models with size assume large groups are perfectly detected while base models overestimate detectability (Figure \@ref(fig:rawDataModels)). As most data was observed in strata 2 which, along with strata 9, were the only areas to exhibit varied group size, we cannot comment too much given extreme heterogeneity.[@rekdal_2014_trends] Binning data results spawned near-identical outcomes to original analysis with less data and thus precluded robust goodness-of-fit testing (Figure \@ref(fig:rawDataModels2)).

In contrast, truncated HN detection functions suggest more extreme detectability underestimates with less extreme size bias. However, the preferred HR function's highly narrow shoulder implies a violation of perfect detectability: the actual fit is driven by just eight observations (Figure \@ref(fig:rawDataModels3)). For cross-data comparability, we thus settle on the suitability of half-normal detection.

```{r rawDataModels, echo = F, results = FALSE, fig.height = 7, fig.cap="Detection functions using original data"}
par(mfrow = c(2,2))
plot(baseRawHN, which=2, pl.col = adjustcolor("steelblue",0.5),border=NULL, lwd = 2, 
     breaks = histBreaks2,
     ylab = "Detection probability (g(x))", xlab = "Distance", las=1,
     main = "Base half-normal model")
# interesting that the covariate model assumes perfect detection of large-sized groups
# this is a quirk of the limited data --> only Region 2 has any within-group variation in size
plot(sizeRawHN, which=2, pl.col = adjustcolor("steelblue",0.5),border=NULL, lwd = 2, 
     breaks = histBreaks2,
     ylab = "Detection probability (g(x))", xlab = "Distance", las=1,
     main = "Half-normal model with covariates")
# both models have solid GOF but the covariate model is slightly better from visual QQ inspection
# get rid of ks = TRUE for  improvements
gof_ds(baseRawHN, main="Base Observed vs. Expected CDF")#, ks = TRUE)
gof_ds(sizeRawHN, main="Size Observed vs. Expected CDF")#, ks = TRUE)
```

```{r rawDataModels2, echo = F, results = FALSE, fig.cap="Detection functions using binned data"}
par(mfrow = c(1,2))
plot(baseBinHN, which=2, pl.col = adjustcolor("steelblue",0.5),border=NULL, lwd = 2, 
     ylab = "Detection probability (g(x))", xlab = "Distance", las=1,
     main = "Base half-normal model")
# interesting that the covariate model assumes perfect detection of large-sized groups
# this is a quirk of the limited data --> only Region 2 has any within-group variation in size
plot(sizeBinHN, which=2, pl.col = adjustcolor("steelblue",0.5),border=NULL, lwd = 2, 
     ylab = "Detection probability (g(x))", xlab = "Distance", las=1,
     main = "Half-normal model with covariates")
```

```{r rawDataModels3, echo = F, results = FALSE, fig.height = 5, fig.cap="Detection functions using truncated data"}
par(mfrow = c(2,3))
plot(baseTruncHN, which=2, pl.col = adjustcolor("steelblue",0.5),border=NULL, lwd = 2, 
     #breaks = histBreaks2,
     ylab = "Detection probability (g(x))", xlab = "Distance", las=1,
     main = "Base half-normal model")
# interesting that the covariate model assumes perfect detection of large-sized groups
# this is a quirk of the limited data --> only Region 2 has any within-group variation in size
plot(sizeTruncHN, which=2, pl.col = adjustcolor("steelblue",0.5),border=NULL, lwd = 2, 
     #breaks = histBreaks2,
     ylab = "Detection probability (g(x))", xlab = "Distance", las=1,
     main = "Half-normal model with covariates")
plot(sizeTruncHR, which=2, pl.col = adjustcolor("steelblue",0.5),border=NULL, lwd = 2, 
     #breaks = histBreaks2,
     ylab = "Detection probability (g(x))", xlab = "Distance", las=1,
     main = "Hazard rate model with covariates")
# both models have solid GOF but the covariate model is slightly better from visual QQ inspection
# get rid of ks = TRUE for  improvements
gof_ds(baseTruncHN, main="Base HN CDF Quantiles")#, ks = TRUE)
gof_ds(sizeTruncHN, main="Covariate HN CDF Quantiles")#, ks = TRUE)
gof_ds(sizeTruncHR, main="Covariate HR CDF Quantiles")#, ks = TRUE)
```

# Discussion

Despite multiple adjustments, we remain concerned about perfect detectability and other DS assumptions. Alternatives like passive monitoring (e.g., using cameras or hydrophones) could improve future surveys. As whales also continually move---behaviourally demonstrating observer avoidance---animals are certainly undetected at their initial locations. 

Non-uniform distributions can be addressed using forward distances in 2D distance sampling.[@borchers_2016_distance] In tandem, we underestimate true abundance due to imperfect detection. Whales are often underwater, likely generating availability bias. The original analysis addresses this using Hidden Markov Models (HMM) to fit a more restrictive model incorporating information on aircraft movement and whale surfacing.[@borchers_2013_using] Though multi-covariate distance sampling provides benefits over conventional distance sampling, we require larger samples and/or more relevant covariates to improve estimates and cope with data heterogeneity and few observations.[@marques_improving_2007]

\newpage
# Code Appendix

```{r fullCode, eval = FALSE, include  = TRUE, echo = TRUE}
library(statsecol)
library(unmarked)
library(tidyverse)
library(MuMIn)
library(AICcmodavg)
library(viridisLite)
library(latex2exp)
# read in data
data(willow)
# prepare the data how I need it
willowNum <- willow %>% mutate(forestsq = forest^2,
                               iLength = 1/length) %>%
  mutate_all(as.numeric) %>% 
  rownames_to_column("id")

willowUnm <- unmarkedFrameOccu(
  y = willowNum[,c("y.1","y.2","y.3")], 
  # we are interested in breeding bird occupancy for the WHOLE breeding season, so days (which affects occupancy on a more granular level) are not good site covariates in this case
  siteCovs = data.frame(elev = willowNum$elev,
                        elev2 = willowNum$elevsq,
                        forest = willowNum$forest,
                        forest2 = willowNum$forestsq,
                        iLength = willowNum$iLength), 
  obsCovs = list(day = willowNum[,c("day1","day2","day3")], 
                 dur = willowNum[,c("dur1","dur2","dur3")],
                 intensity = willowNum[,c("intensity1","intensity2","intensity3")],
                 length = willowNum[,c("length","length","length")],
                 iLength = willowNum[,c("iLength","iLength","iLength")],
                 forest = willowNum[,c("forest","forest","forest")],
                 forest2 = willowNum[,c("forestsq","forestsq","forestsq")],
                 elev = willowNum[,c("elev","elev","elev")],
                 elev2 = willowNum[,c("elevsq","elevsq","elevsq")])
)
summary(willowUnm)

# null model
m0 <- occu(~1 ~1, data = willowUnm)

# full model
# intensity:length interaction is just dur and thus not included
mfull <- occu(formula = ~day + dur + intensity + length + day*dur + dur*intensity + day*length + day*intensity + dur*length  # p formula
              ~elev + elev2 + forest + forest2 + elev*forest + elev2*forest2 + elev*forest2 + elev2*forest, #psi formula
              data = willowUnm) #the data object

summary(mfull)
  
# all three agree on a few things
# elev2 and elev:forest are mutually exclusive
# p(day) is not worth including
# all p interaction terms are useless
mDredgeB <- dredge(mfull, rank = "BIC")
mDredgeA <- dredge(mfull, rank = "AIC")
mDredgeAc <- dredge(mfull, rank = "AICc")

mBIC <- occu(formula = ~dur # p formula
             ~elev + forest + forest^2 + elev*forest, #psi formula
             data = willowUnm) #the data object
summary(mBIC)

mfullAct <- occu(formula = ~day + dur + intensity + length + day*dur + dur*intensity + day*length + day*intensity + dur*length + forest + forest*dur + forest*day + forest*intensity + forest*length # p formula
              ~elev + elev2 + forest + forest2 + elev*forest + elev2*forest2 + elev*forest2 + elev2*forest, #psi formula
              data = willowUnm) #the data object

summary(mfullAct)

# all three agree on a few things
# elev2 and elev:forest are mutually exclusive
# p(day) is not worth including
# all p interaction terms are useless
mDredgeB2 <- dredge(mfullAct, rank = "BIC")
mDredgeA2 <- dredge(mfullAct, rank = "AIC")
mDredgeAc2 <- dredge(mfullAct, rank = "AICc")

mAlt <- occu(formula = ~day + dur + forest  # p formula
              ~elev + forest, #psi formula
              data = willowUnm) #the data object

mAlt1 <- occu(formula = ~day + dur + forest # p formula
              ~elev + elev2 + forest + forest2, #psi formula
              data = willowUnm) #the data object

mAlt2 <- occu(formula = ~day + dur + forest + forest2 # p formula
              ~elev + elev2 + forest + forest2, #psi formula
              data = willowUnm) #the data object

mOptm_Alt <- occu(formula = ~day + dur + forest + forest*day # p formula
              ~elev, #psi formula
              data = willowUnm) #the data object

# recording visibility/leafing may be a good way to increase separability and avoid forest in both models
mOptm <- occu(formula = ~day + dur + forest + forest*day # p formula
              ~elev + elev2 + forest + forest2 + elev2*forest + elev2*forest2 , #psi formula
              data = willowUnm) #the data object

mAlt3 <- occu(formula = ~day + dur + forest + forest*day # p formula
              ~elev + elev2 + forest + forest2 + elev*forest + elev*forest2 + elev2*forest + elev2*forest2 , #psi formula
              data = willowUnm) #the data object

# combine some representative models for displaying
fl <- fitList(
  "p(.)                                   psi(.)"                                                                      = m0,
  "p(day + dur + forest)                  psi(elev + forest)"                                                          = mAlt,
  "p(day + dur + forest)                  psi(elev + elev^2 + forest + forest^2)"                                      = mAlt1,
  "p(day + dur + forest + forest^2)       psi(elev + elev^2 + forest + forest^2)"                                      = mAlt2,
  "p(day + dur + forest + forest*day)     psi(elev)"                                                                   = mOptm_Alt,  
  "p(day + dur + forest + forest*day)     psi(elev + elev^2*(forest + forest^2))"                                      = mOptm,
  "p(day + dur + forest + forest*day)     psi((elev + elev^2)*(forest + forest^2))"                                    = mAlt3)

# model output table to format
ms <- modSel(fl)

# actual model summary
summary(mOptm)

# test for VIF?
vif(mOptm, type = "state")
vif(mOptm, type = "det")

# GOF for best model....it's meh
gof.boot <- mb.gof.test(mOptm, nsim = 1000, ncores = 10)
ggplot() + 
  geom_histogram(data = data.frame(t.star = gof.boot$t.star),
                 aes(x=t.star), color="black", fill="#27813E", alpha = 0.3) +
  geom_vline(aes(xintercept = gof.boot$chi.square), linewidth = 0.8, color = "maroon") +
  xlab("Peasron Chi-square statistic") + theme_bw() +
  ggtitle(bquote("Bootstrapped"~chi^2~"fit statistic (1000 samples, p = 0.071)"))

# some plotting of results
data(Switzerland)
gelev <- ggplot(data = Switzerland, aes(x=x, y=y,fill=elevation)) +
  geom_raster() +
  scale_fill_viridis_c(direction = 1, 
                       option = "B") +
  theme_bw() +
  theme(axis.text = element_blank(),
        axis.line = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        panel.border = element_blank(),
        legend.position = "bottom") + 
  labs(x = "",
       y = "",
       fill = "Elevation (m)") + 
  guides(fill = guide_colorbar(# draw border around the legend
                               frame.colour = "black",
                               barwidth = 10)) + 
  coord_fixed()
gfor <- ggplot(data = Switzerland, aes(x=x, y=y,fill=forest)) +
  geom_raster() +
  scale_fill_viridis_c(direction = 1, 
                       option = "D") +
  theme_bw() +
  theme(axis.text = element_blank(),
        axis.line = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        panel.border = element_blank(),
        legend.position="bottom") + 
  labs(x = "",
       y = "",
       fill = "Forest (%)")  + 
  guides(fill = guide_colorbar(# draw border around the legend
    frame.colour = "black",
    barwidth = 10)) + 
  coord_fixed()

for_pred <- data.frame(elev = (Switzerland$elevation - 1182.574)/646.333,      # convert original m to z-score
                       elev2 = ((Switzerland$elevation - 1182.574)/646.333)^2, # convert original m to z-score
                       forest = Switzerland$forest/100,          #want prop not %
                       forest2 = Switzerland$forest/100,         #want prop not %
                       X = Switzerland$x,                        #keep the coordinates
                       Y = Switzerland$y)                        #keep the coordinates
cowplot::plot_grid(gelev,gfor,nrow=2)

willowPredSDM <- modavgPred(list(mOptm), # top model
                            newdata = for_pred, #spatially indexed data frame
                            parm.type = "psi",  #predict from state model
                            c.hat = gof.boot$c.hat.est) #inflate SEs using Royle & Kery method
#add data to predictions manually
willow_sdm <- for_pred %>% mutate(Predicted = willowPredSDM$mod.avg.pred,
                                  SE = willowPredSDM$uncond.se,
                                  lower = willowPredSDM$lower.CL,
                                  upper = willowPredSDM$upper.CL)

gpredM_1 <- ggplot(data = willow_sdm, aes(x=X, y=Y,fill=Predicted)) +
  geom_raster() +
  scale_fill_viridis_c(direction = 1, 
                       option = "H") +
  # add actual observations if we have x,y data
  # geom_point(data = willowNum, aes(x=X, y=Y)) +
  theme_bw() +
  theme(axis.text = element_blank(),
        axis.line = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        panel.border = element_blank(),
        legend.position="bottom") + 
  labs(x = "",
       y = "",
       fill = TeX(r'(Estimated $\psi$)'))  + 
  guides(fill = guide_colorbar(# draw border around the legend
    frame.colour = "black",
    barwidth = 10)) + 
  coord_fixed()

gpredE <- ggplot(data = willow_sdm, aes(x=X, y=Y,fill=SE)) +
  geom_raster() +
  scale_fill_viridis_c(direction = 1, 
                       option = "H") +
  theme_bw() +
  theme(axis.text = element_blank(),
        axis.line = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        panel.border = element_blank(),
        legend.position="bottom") + 
  labs(x = "",
       y = "",
       fill = TeX(r'(Estimated $\psi$ Error)'))  + 
  guides(fill = guide_colorbar(# draw border around the legend
    frame.colour = "black",
    barwidth = 10)) + 
  coord_fixed()

cowplot::plot_grid(gelev,gfor,gpredM_1,gpredE,nrow=2)

gpredL <- ggplot(data = willow_sdm, aes(x=X, y=Y,fill=lower)) +
  geom_raster() +
  scale_fill_viridis_c(direction = 1, 
                       option = "H", 
                       limits= c(0,1)) +
  theme_bw() +
  theme(axis.text = element_blank(),
        axis.line = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        panel.border = element_blank(),
        legend.position="left") + 
  labs(x = "",
       y = "",
       fill = TeX(r'(Lower Bound $\psi$)')) + 
  guides(fill = guide_colorbar(# draw border around the legend
    frame.colour = "black")) + 
  coord_fixed()

gpredM_2 <- ggplot(data = willow_sdm, aes(x=X, y=Y,fill=Predicted)) +
  geom_raster() +
  scale_fill_viridis_c(direction = 1, 
                       option = "H", 
                       limits= c(0,1)) +
  # add actual observations if we have x,y data
  # geom_point(data = willowNum, aes(x=X, y=Y)) +
  theme_bw() +
  theme(axis.text = element_blank(),
        axis.line = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        panel.border = element_blank(),
        legend.position="left") + 
  labs(x = "",
       y = "",
       fill = TeX(r'(Estimated $\psi$)'))  + 
  guides(fill = guide_colorbar(# draw border around the legend
    frame.colour = "black")) + 
  coord_fixed()

gpredH <- ggplot(data = willow_sdm, aes(x=X, y=Y,fill=upper)) +
  geom_raster() +
  scale_fill_viridis_c(direction = 1, 
                       option = "H", 
                       limits= c(0,1)) +
  theme_bw() +
  theme(axis.text = element_blank(),
        axis.line = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        panel.border = element_blank(),
        legend.position="left") + 
  labs(x = "",
       y = "",
       fill = TeX(r'(Upper Bound $\psi$)'))  + 
  guides(fill = guide_colorbar(# draw border around the legend
    frame.colour = "black")) + 
  coord_fixed()

cowplot::plot_grid(gpredL, gpredM_2, gpredH,nrow=3)



#---------------------------------------------------------------------------------------#
#psi ~ elev | mean(forest)
pred_psi_eleL <- data.frame(elev = seq(min(willowUnm@siteCovs$elev, na.rm=TRUE),
                                      max(willowUnm@siteCovs$elev, na.rm=TRUE),
                                      length = 30),
                           forest = quantile(probs = 0.25, willowUnm@siteCovs$forest, na.rm=TRUE)) %>% 
  mutate(elev2 = elev^2,
         forest2 = forest^2)
predPsiEleL <- modavgPred(list(mOptm), newdata = pred_psi_eleL, parm.type = "psi", c.hat = gof.boot$c.hat.est)
pred_psi_eleL <- pred_psi_eleL %>% mutate(Predicted = predPsiEleL$mod.avg.pred,
                                          SE = predPsiEleL$uncond.se,
                                          lower = predPsiEleL$lower.CL,
                                          upper = predPsiEleL$upper.CL,
                                          elevR = 1182.574 + elev*646.333)
ggpsieleL <- ggplot(data = pred_psi_eleL, aes(x = elevR, y = Predicted)) +
  geom_ribbon(aes(ymin=lower, ymax=upper), fill="#440154", alpha=0.1) +
  geom_line(size=1,color="#440154") +
  ylab("P(Occupied)") + xlab("Elevation (m)") + ylim(0,1) + theme_bw()

pred_psi_eleM <- data.frame(elev = seq(min(willowUnm@siteCovs$elev, na.rm=TRUE),
                                      max(willowUnm@siteCovs$elev, na.rm=TRUE),
                                      length = 30),
                           forest = median(willowUnm@siteCovs$forest, na.rm=TRUE)) %>% 
  mutate(elev2 = elev^2,
         forest2 = forest^2)
predPsiEleM <- modavgPred(list(mOptm), newdata = pred_psi_eleM, parm.type = "psi", c.hat = gof.boot$c.hat.est)
pred_psi_eleM <- pred_psi_eleM %>% mutate(Predicted = predPsiEleM$mod.avg.pred,
                                          SE = predPsiEleM$uncond.se,
                                          lower = predPsiEleM$lower.CL,
                                          upper = predPsiEleM$upper.CL,
                                          elevR = 1182.574 + elev*646.333)
ggpsieleM <- ggplot(data = pred_psi_eleM, aes(x = elevR, y = Predicted)) +
  geom_ribbon(aes(ymin=lower, ymax=upper), fill="#440154", alpha=0.1) +
  geom_line(size=1,color="#440154") +
  ylab("P(Occupied)") + xlab("Elevation (m)") + ylim(0,1) + theme_bw()

pred_psi_eleH <- data.frame(elev = seq(min(willowUnm@siteCovs$elev, na.rm=TRUE),
                                      max(willowUnm@siteCovs$elev, na.rm=TRUE),
                                      length = 30),
                           forest = quantile(probs = 0.75, willowUnm@siteCovs$forest, na.rm=TRUE)) %>% 
  mutate(elev2 = elev^2,
         forest2 = forest^2)
predPsiEleH <- modavgPred(list(mOptm), newdata = pred_psi_eleH, parm.type = "psi", c.hat = gof.boot$c.hat.est)
pred_psi_eleH <- pred_psi_eleH %>% mutate(Predicted = predPsiEleH$mod.avg.pred,
                                          SE = predPsiEleH$uncond.se,
                                          lower = predPsiEleH$lower.CL,
                                          upper = predPsiEleH$upper.CL,
                                          elevR = 1182.574 + elev*646.333)
ggpsieleH <- ggplot(data = pred_psi_eleH, aes(x = elevR, y = Predicted)) +
  geom_ribbon(aes(ymin=lower, ymax=upper), fill="#440154", alpha=0.1) +
  geom_line(size=1,color="#440154") +
  ylab("P(Occupied)") + xlab("Elevation (m)") + ylim(0,1) + theme_bw()

#---------------------------------------------------------------------------------------#
#psi ~ for | mean(elev)
pred_psi_forL <- data.frame(forest = seq(min(willowUnm@siteCovs$forest, na.rm=TRUE),
                                      max(willowUnm@siteCovs$forest, na.rm=TRUE),
                                      length = 30),
                           elev = quantile(probs = 0.25, willowUnm@siteCovs$elev, na.rm=TRUE)) %>% 
  mutate(elev2 = elev^2,
         forest2 = forest^2)
predPsiForL <- modavgPred(list(mOptm), newdata = pred_psi_forL, parm.type = "psi", c.hat = gof.boot$c.hat.est)
pred_psi_forL <- pred_psi_forL %>% mutate(Predicted = predPsiForL$mod.avg.pred,
                                          SE = predPsiForL$uncond.se,
                                          lower = predPsiForL$lower.CL,
                                          upper = predPsiForL$upper.CL,
                                          forestP = forest*100)
ggpsiforL <- ggplot(data = pred_psi_forL, aes(x = forestP, y = Predicted)) +
  geom_ribbon(aes(ymin=lower, ymax=upper), fill="#440154", alpha=0.1) +
  geom_line(size=1,color="#440154") +
  ylab("P(Occupied)") + xlab("Forest Cover (%)") + ylim(0,1) + theme_bw()

pred_psi_forM <- data.frame(forest = seq(min(willowUnm@siteCovs$forest, na.rm=TRUE),
                                        max(willowUnm@siteCovs$forest, na.rm=TRUE),
                                        length = 30),
                           elev = median(willowUnm@siteCovs$elev, na.rm=TRUE)) %>% 
  mutate(elev2 = elev^2,
         forest2 = forest^2)
predPsiForM <- modavgPred(list(mOptm), newdata = pred_psi_forM, parm.type = "psi", c.hat = gof.boot$c.hat.est)
pred_psi_forM <- pred_psi_forM %>% mutate(Predicted = predPsiForM$mod.avg.pred,
                                          SE = predPsiForM$uncond.se,
                                          lower = predPsiForM$lower.CL,
                                          upper = predPsiForM$upper.CL,
                                          forestP = forest*100)
ggpsiforM <- ggplot(data = pred_psi_forM, aes(x = forestP, y = Predicted)) +
  geom_ribbon(aes(ymin=lower, ymax=upper), fill="#440154", alpha=0.1) +
  geom_line(size=1,color="#440154") +
  ylab("P(Occupied)") + xlab("Forest Cover (%)") + ylim(0,1) + theme_bw()

pred_psi_forH <- data.frame(forest = seq(min(willowUnm@siteCovs$forest, na.rm=TRUE),
                                        max(willowUnm@siteCovs$forest, na.rm=TRUE),
                                        length = 30),
                           elev = quantile(probs = 0.75, willowUnm@siteCovs$elev, na.rm=TRUE)) %>% 
  mutate(elev2 = elev^2,
         forest2 = forest^2)
predPsiForH <- modavgPred(list(mOptm), newdata = pred_psi_forH, parm.type = "psi", c.hat = gof.boot$c.hat.est)
pred_psi_forH <- pred_psi_forH %>% mutate(Predicted = predPsiForH$mod.avg.pred,
                                    SE = predPsiForH$uncond.se,
                                    lower = predPsiForH$lower.CL,
                                    upper = predPsiForH$upper.CL,
                                    forestP = forest*100)
ggpsiforH <- ggplot(data = pred_psi_forH, aes(x = forestP, y = Predicted)) +
  geom_ribbon(aes(ymin=lower, ymax=upper), fill="#440154", alpha=0.1) +
  geom_line(size=1,color="#440154") +
  ylab("P(Occupied)") + xlab("Forest Cover (%)") + ylim(0,1) + theme_bw()

cowplot::plot_grid(ggpsieleL, ggpsieleM, ggpsieleH, ggpsiforL, ggpsiforM, ggpsiforH, nrow=2)















# generate predictions for 4 quadrants of interest
willowPred <- willowNum %>% filter(id %in% c(25, 62, 150, 203)) %>% 
  rename(elev2 = elevsq,
         forest2 = forestsq)
# predicting occurrence from the state process
predQuads <- modavgPred(list(mOptm), newdata = willowPred, parm.type = "psi", c.hat = gof.boot$c.hat.est)
willowRes <- willowPred %>% mutate(Predicted = predQuads$mod.avg.pred,
                                   SE = predQuads$uncond.se,
                                   lower = predQuads$lower.CL,
                                   upper = predQuads$upper.CL,
                                   elev = round((646.333*elev+1182.574),0)) %>% 
  select(-c(elev2, forest2))
```

\newpage
# References  
